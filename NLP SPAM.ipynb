{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d422395",
   "metadata": {},
   "source": [
    "## Proyecto NLP para reconocer mensajerías tipo SPAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e86f47f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/home/mauricio/spark-3.2.1-bin-hadoop3.2\")\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d3f7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 17:58:15 WARN Utils: Your hostname, mauricio-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "22/09/03 17:58:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/mauricio/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/09/03 17:58:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/09/03 17:58:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "SPARK = SparkSession.builder.appName(\"ProyectoNLP\").getOrCreate()\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc992b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (Tokenizer, RegexTokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer,NGram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18bbdf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,udf, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a09690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6df1912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:===========================================================(1 + 0) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = SPARK.read.csv(\"/home/mauricio/curso/Spark_for_Machine_Learning/Natural_Language_Processing/smsspamcollection/SMSSpamCollection\",\n",
    "                   inferSchema=True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c25e8b",
   "metadata": {},
   "source": [
    "### Crear una linea base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1bc3ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| _c0|                 _c1|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "|spam|Free entry in 2 a...|\n",
      "+----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4beb783d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5574, 2)\n"
     ]
    }
   ],
   "source": [
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b55ee88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Renombrar columnas\n",
    "df = df.withColumnRenamed(\"_c0\",\"class\").withColumnRenamed(\"_c1\",\"response_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "825d108b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|       response_text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "+-----+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "a53523f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Se crea el Pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"response_text\" ,outputCol=\"token_text\")\n",
    "stop_remove=StopWordsRemover(inputCol=\"token_text\" ,outputCol=\"stop_text\")\n",
    "count_vect=CountVectorizer(inputCol=\"stop_text\" ,outputCol=\"c_vec\")\n",
    "idf=IDF(inputCol=\"c_vec\" ,outputCol=\"tf_idf\")\n",
    "\n",
    "flag_nflag_numeric=StringIndexer(inputCol=\"class\" ,outputCol=\"label\") ### Label Encoder ( 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "23869f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "4235a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eliges que Features (x's) usar en el modelo para entrenarlo\n",
    "clean_up = VectorAssembler(inputCols=[\"tf_idf\"],outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "647aca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "ade3d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb  = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "62933045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "fffd5b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creas el Pipeline, que te hace datacleanin, tokeniza y vectoriza, etc.\n",
    "data_prep_pipe = Pipeline(stages = [flag_nflag_numeric,tokenizer,stop_remove,count_vect,\n",
    "                          idf,clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "b17a1f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = data_prep_pipe.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "f23a7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = cleaner.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "352c7141",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data.select(\"label\",\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "926c430e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(13423,[7,11,31,6...|\n",
      "|  0.0|(13423,[0,24,297,...|\n",
      "|  1.0|(13423,[2,13,19,3...|\n",
      "|  0.0|(13423,[0,70,80,1...|\n",
      "+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "9f9139fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training,test=clean_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "020aa0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:23:07 WARN DAGScheduler: Broadcasting large task binary with size 1141.3 KiB\n",
      "22/09/03 22:23:07 WARN DAGScheduler: Broadcasting large task binary with size 1125.6 KiB\n"
     ]
    }
   ],
   "source": [
    "flag_detector = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "692a8473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- response_text: string (nullable = true)\n",
      " |-- length: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "ff35c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = flag_detector.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "83514c15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:23:07 WARN DAGScheduler: Broadcasting large task binary with size 1358.8 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13423,[0,1,2,7,8...|[-671.15936157306...|[1.0,6.8628272745...|       0.0|\n",
      "|  0.0|(13423,[0,1,2,41,...|[-928.38226325591...|[1.0,1.5089958811...|       0.0|\n",
      "|  0.0|(13423,[0,1,7,8,1...|[-1002.1610889989...|[1.0,6.4086836214...|       0.0|\n",
      "|  0.0|(13423,[0,1,7,15,...|[-539.30905223688...|[1.0,2.1955725259...|       0.0|\n",
      "|  0.0|(13423,[0,1,14,78...|[-573.53850329822...|[1.0,1.8668975625...|       0.0|\n",
      "|  0.0|(13423,[0,1,18,20...|[-729.08725061967...|[1.0,2.1655579625...|       0.0|\n",
      "|  0.0|(13423,[0,1,21,27...|[-617.06976453005...|[1.0,4.7987801895...|       0.0|\n",
      "|  0.0|(13423,[0,1,23,63...|[-1109.8093978057...|[1.0,4.7821200902...|       0.0|\n",
      "|  0.0|(13423,[0,1,24,31...|[-274.58074613916...|[1.0,8.1731217499...|       0.0|\n",
      "|  0.0|(13423,[0,1,874],...|[-77.447369573634...|[0.99999996161854...|       0.0|\n",
      "|  0.0|(13423,[0,2,3,5,6...|[-2180.6917385490...|[1.0,3.3218252303...|       0.0|\n",
      "|  0.0|(13423,[0,2,3,5,3...|[-431.84529721702...|[1.0,5.2345186154...|       0.0|\n",
      "|  0.0|(13423,[0,2,3,6,9...|[-2763.9314166407...|[1.0,1.1378498254...|       0.0|\n",
      "|  0.0|(13423,[0,2,3,8,2...|[-1388.4121581867...|[1.0,1.0287205471...|       0.0|\n",
      "|  0.0|(13423,[0,2,4,5,7...|[-820.58384882179...|[1.0,8.8835475145...|       0.0|\n",
      "|  0.0|(13423,[0,2,4,8,2...|[-459.61754500251...|[1.0,1.1980473977...|       0.0|\n",
      "|  0.0|(13423,[0,2,4,10,...|[-1052.1731777992...|[1.0,7.9119012778...|       0.0|\n",
      "|  0.0|(13423,[0,2,4,40,...|[-1374.7176226785...|[0.99999982861955...|       0.0|\n",
      "|  0.0|(13423,[0,2,4,44,...|[-1699.6060520526...|[1.0,1.3329786610...|       0.0|\n",
      "|  0.0|(13423,[0,2,5,8,4...|[-714.74556729896...|[1.0,3.9591432568...|       0.0|\n",
      "|  0.0|(13423,[0,2,7,8,1...|[-389.53452107002...|[1.0,2.9269051021...|       0.0|\n",
      "|  0.0|(13423,[0,2,7,43,...|[-490.41164992818...|[1.0,2.4291676843...|       0.0|\n",
      "|  0.0|(13423,[0,2,8,28,...|[-1142.2820737505...|[1.0,1.4176891860...|       0.0|\n",
      "|  0.0|(13423,[0,2,8,75,...|[-617.66996484547...|[1.0,2.5142250009...|       0.0|\n",
      "|  0.0|(13423,[0,2,10,24...|[-618.92209147809...|[1.0,3.1365624053...|       0.0|\n",
      "|  0.0|(13423,[0,2,14,24...|[-366.26268447175...|[0.99999999998859...|       0.0|\n",
      "|  0.0|(13423,[0,2,14,31...|[-867.01581440471...|[1.0,1.1759686411...|       0.0|\n",
      "|  0.0|(13423,[0,2,14,13...|[-220.90500864016...|[1.0,6.8262302502...|       0.0|\n",
      "|  0.0|(13423,[0,2,15,52...|[-1800.0710233841...|[1.0,1.5268272672...|       0.0|\n",
      "|  0.0|(13423,[0,2,18,30...|[-708.20648102407...|[0.99999999768752...|       0.0|\n",
      "|  0.0|(13423,[0,2,20,10...|[-326.19208965078...|[1.0,1.5255322107...|       0.0|\n",
      "|  0.0|(13423,[0,2,21,60...|[-523.34924891723...|[0.99999937637204...|       0.0|\n",
      "|  0.0|(13423,[0,2,21,62...|[-317.88934817267...|[1.0,3.8248983247...|       0.0|\n",
      "|  0.0|(13423,[0,2,23,10...|[-413.82950087035...|[1.0,2.6519598290...|       0.0|\n",
      "|  0.0|(13423,[0,2,23,71...|[-399.15617723772...|[1.0,3.7006479813...|       0.0|\n",
      "|  0.0|(13423,[0,2,27,57...|[-589.82773487271...|[1.0,7.3940224082...|       0.0|\n",
      "|  0.0|(13423,[0,2,35,73...|[-1062.8089346374...|[0.99999999999666...|       0.0|\n",
      "|  0.0|(13423,[0,2,39,10...|[-1017.9904681549...|[1.0,2.5721685100...|       0.0|\n",
      "|  0.0|(13423,[0,2,44,69...|[-1293.0301329661...|[0.99999999940920...|       0.0|\n",
      "|  0.0|(13423,[0,2,71,18...|[-574.52363412819...|[1.0,8.1457782642...|       0.0|\n",
      "|  0.0|(13423,[0,2,128,1...|[-486.21277101221...|[1.0,3.0571054149...|       0.0|\n",
      "|  0.0|(13423,[0,2,359,5...|[-166.43606119120...|[0.99999207905893...|       0.0|\n",
      "|  0.0|(13423,[0,3,6,8,1...|[-3140.9162820432...|[1.0,8.9552186611...|       0.0|\n",
      "|  0.0|(13423,[0,3,6,9,3...|[-1834.1131548736...|[1.0,2.0689919573...|       0.0|\n",
      "|  0.0|(13423,[0,3,29,78...|[-444.20742724886...|[1.0,7.1686026180...|       0.0|\n",
      "|  0.0|(13423,[0,4,5,8,2...|[-703.97473034294...|[1.0,4.4036183809...|       0.0|\n",
      "|  0.0|(13423,[0,4,7,8,2...|[-1343.4942154084...|[0.37772648837925...|       1.0|\n",
      "|  0.0|(13423,[0,4,7,27,...|[-2834.0482752782...|[1.0,1.0383393838...|       0.0|\n",
      "|  0.0|(13423,[0,4,12,15...|[-1103.1714058387...|[1.0,4.1148080548...|       0.0|\n",
      "|  0.0|(13423,[0,4,12,27...|[-1528.0538278388...|[2.01628903934905...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Observamos como predice\n",
    "test_results.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "6b887b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Observaremos Accuracy, f1, etc.\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "acc_eval=MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "ebb7461e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:23:08 WARN DAGScheduler: Broadcasting large task binary with size 1363.7 KiB\n"
     ]
    }
   ],
   "source": [
    "ff1 = acc_eval.evaluate(test_results,{acc_eval.metricName: \"f1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31bd5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Obtenemos un f1 de 92%\n",
    "print(ff1)\n",
    "### Este será nuestra linea base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "4bda93b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:23:08 WARN DAGScheduler: Broadcasting large task binary with size 1363.7 KiB\n",
      "22/09/03 22:23:08 WARN DAGScheduler: Broadcasting large task binary with size 1363.7 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9114990969295605"
      ]
     },
     "execution_count": 708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator= MulticlassClassificationEvaluator()\n",
    "\n",
    "evaluator.setPredictionCol(\"prediction\")\n",
    "\n",
    "evaluator.evaluate(test_results)\n",
    "\n",
    "evaluator.evaluate(test_results, {evaluator.metricName: \"accuracy\"})\n",
    "### Un accuracy del 91"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f648b2",
   "metadata": {},
   "source": [
    "### La matriz de confusión de abajo es de NBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "e8919a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:23:09 WARN DAGScheduler: Broadcasting large task binary with size 1352.9 KiB\n",
      "22/09/03 22:23:09 WARN DAGScheduler: Broadcasting large task binary with size 1354.1 KiB\n",
      "22/09/03 22:23:09 WARN DAGScheduler: Broadcasting large task binary with size 1340.5 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1306.  140.]\n",
      " [   7.  208.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:23:10 WARN DAGScheduler: Broadcasting large task binary with size 1350.6 KiB\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "preds_and_labels = test_results.select(['prediction',\n",
    "                                       'label']).withColumn('label',\n",
    "                                                        F.col('label').cast(FloatType())).orderBy('prediction')\n",
    "\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "### Califica a los 0 con un 96% y a los 1 con un 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "id": "0cd838a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:27:02 WARN DAGScheduler: Broadcasting large task binary with size 1139.1 KiB\n",
      "22/09/03 22:27:03 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:03 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:04 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:04 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:04 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:04 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:04 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:05 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:05 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:05 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:05 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:06 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:06 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:06 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:08 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:08 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:08 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:08 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:09 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:09 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:09 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:10 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:10 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:10 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:10 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:11 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:11 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:11 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:11 WARN DAGScheduler: Broadcasting large task binary with size 1140.7 KiB\n",
      "22/09/03 22:27:12 WARN DAGScheduler: Broadcasting large task binary with size 1265.1 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9858046481960333"
      ]
     },
     "execution_count": 716,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Intentando con Log Reg\n",
    "\n",
    "training,test=clean_data.randomSplit([0.7,0.3])\n",
    "flag_detector = lr.fit(training)\n",
    "test_results = flag_detector.transform(test)\n",
    "ff1 = acc_eval.evaluate(test_results,{acc_eval.metricName: \"f1\"})\n",
    "ff1\n",
    "## F1 de 98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "id": "ca672197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:27:12 WARN DAGScheduler: Broadcasting large task binary with size 1265.1 KiB\n",
      "22/09/03 22:27:13 WARN DAGScheduler: Broadcasting large task binary with size 1265.1 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9859484777517564"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator= MulticlassClassificationEvaluator()\n",
    "\n",
    "evaluator.setPredictionCol(\"prediction\")\n",
    "\n",
    "evaluator.evaluate(test_results)\n",
    "\n",
    "evaluator.evaluate(test_results, {evaluator.metricName: \"accuracy\"})\n",
    "## Accuracy del 98%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce44496",
   "metadata": {},
   "source": [
    "### La matriz de confusion de abajo es de Regresión Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "id": "8d908d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:27:13 WARN DAGScheduler: Broadcasting large task binary with size 1254.2 KiB\n",
      "22/09/03 22:27:13 WARN DAGScheduler: Broadcasting large task binary with size 1255.4 KiB\n",
      "22/09/03 22:27:14 WARN DAGScheduler: Broadcasting large task binary with size 1241.9 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1486.    7.]\n",
      " [  17.  198.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:27:14 WARN DAGScheduler: Broadcasting large task binary with size 1251.7 KiB\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "preds_and_labels = test_results.select(['prediction',\n",
    "                                       'label']).withColumn('label',\n",
    "                                                        F.col('label').cast(FloatType())).orderBy('prediction')\n",
    "\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "### Predice un poco mejor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53a4e9",
   "metadata": {},
   "source": [
    "## Pasando a hacer un poco de Ingeniería de Data y EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a32fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos Data Exploration\n",
    "df =  df.withColumn(\"length\", length(df[\"_c1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba6144c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------+\n",
      "| _c0|                 _c1|length|\n",
      "+----+--------------------+------+\n",
      "| ham|Go until jurong p...|   111|\n",
      "| ham|Ok lar... Joking ...|    29|\n",
      "|spam|Free entry in 2 a...|   155|\n",
      "| ham|U dun say so earl...|    49|\n",
      "+----+--------------------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b76f1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|response_text                                                                                                                                                                                       |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                                                                     |\n",
      "|Ok lar... Joking wif u oni...                                                                                                                                                                       |\n",
      "|Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's                                         |\n",
      "|U dun say so early hor... U c already then say...                                                                                                                                                   |\n",
      "|Nah I don't think he goes to usf, he lives around here though                                                                                                                                       |\n",
      "|FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv                                                 |\n",
      "|Even my brother is not like to speak with me. They treat me like aids patent.                                                                                                                       |\n",
      "|As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune                                    |\n",
      "|WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.                                       |\n",
      "|Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030                                          |\n",
      "|I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.                                                                                       |\n",
      "|SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info                                                            |\n",
      "|URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18                                         |\n",
      "|I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.|\n",
      "|I HAVE A DATE ON SUNDAY WITH WILL!!                                                                                                                                                                 |\n",
      "|XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL                                               |\n",
      "|Oh k...i'm watching here:)                                                                                                                                                                          |\n",
      "|Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.                                                                                                                   |\n",
      "|Fine if thats the way u feel. Thats the way its gota b                                                                                                                                            |\n",
      "|England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/ú1.20 POBOXox36504W45WQ 16+                                         |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"response_text\").show(truncate=False)\n",
    "## Vemos que hay textos con secuencias de caracteres sin sentido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4116a253",
   "metadata": {},
   "source": [
    "#### Para limpiar un poco la data del texto, quizá podemos enviarlo todo a lower case, quitar comas, puntos y comillas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df40efc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Observemos si hay data inútiles\n",
    "from pyspark.sql.functions import col,length\n",
    "\n",
    "## Uso esta linea para ver las palabras\n",
    "#df.select(\"response_text\").where(length(col(\"response_text\"))<10).show(truncate=False)\n",
    "\n",
    "## Luego esta linea para contar\n",
    "df.select(\"response_text\").where(length(col(\"response_text\"))<10).count()\n",
    "## parece que las que son menores de 10 letras no tienen mucha utilidad y son 56 rows (no quitamos mucha data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eed2e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  df.withColumn(\"length\", length(df[\"response_text\"]))\n",
    "df_filtrado = df.filter(df.length>10)\n",
    "## Filtramos y nos quedamos con las que tengan mas de 10 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7e31913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|       response_text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "|  ham|U dun say so earl...|    49|\n",
      "+-----+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtrado.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35615033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|class|      avg(length)|\n",
      "+-----+-----------------+\n",
      "|  ham| 83.7940640458214|\n",
      "| spam|139.3028263795424|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtrado.groupBy(\"class\").mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eee345",
   "metadata": {},
   "source": [
    "#### Vemos que la mayoría de los mensajes marcados como SPAM suelen tener un AVG de 139 caracterres, \n",
    "#### eso es el casi el doble de los que no están marcados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10f3785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LISTA = [\"/\",\"@\",\"*\",\"+\",\"-\",\")\",\"(\",\"&\",\"$\",\"#\",\"!\"]\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "newDF = df_filtrado.withColumn(\"response_text\",regexp_replace(\"response_text\",\"'\",\"\"))\n",
    "newDF = newDF.withColumn(\"response_text\",regexp_replace(\"response_text\",\"@\",\"\"))\n",
    "newDF = newDF.withColumn(\"response_text\",regexp_replace(\"response_text\",\"/\",\"\"))\n",
    "## Remover caracteres inputiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "358cf371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|       response_text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "|  ham|U dun say so earl...|    49|\n",
      "+-----+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21bb46d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "# Se hace un lwoercase del texto\n",
    "NDF = newDF.withColumn(\"response_text\",f.lower(\"response_text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4f73c5b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|       response_text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|go until jurong p...|   111|\n",
      "| spam|free entry in 2 a...|   155|\n",
      "|  ham|u dun say so earl...|    49|\n",
      "+-----+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "bf7b0a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = NDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "c20544e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Para poder contar palabras y stopwords tenemos que tokenizar primero\n",
    "tokenizer = Tokenizer(inputCol=\"response_text\" ,outputCol=\"token_text\") \n",
    "tokenized = tokenizer.transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "dd6e07e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+--------------------+\n",
      "|class|       response_text|length|          token_text|\n",
      "+-----+--------------------+------+--------------------+\n",
      "|  ham|go until jurong p...|   111|[go, until, juron...|\n",
      "| spam|free entry in 2 a...|   155|[free, entry, in,...|\n",
      "|  ham|u dun say so earl...|    49|[u, dun, say, so,...|\n",
      "+-----+--------------------+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "58a79ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, explode\n",
    "\n",
    "q_palabras = tokenized.select(\"*\", explode(\"token_text\").alias(\"exploded\")).groupBy(\"class\", \"response_text\",\"token_text\",\"length\").agg(count(\"exploded\").alias(\"palabras\"))\n",
    " \n",
    "#Ya tengo la cantidad de palabras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "791bac85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+------+--------+\n",
      "|class|       response_text|          token_text|length|palabras|\n",
      "+-----+--------------------+--------------------+------+--------+\n",
      "| spam|want 2 get laid t...|[want, 2, get, la...|   162|      28|\n",
      "| spam|call germany for ...|[call, germany, f...|   128|      24|\n",
      "|  ham|i taught that ran...|[i, taught, that,...|   143|      27|\n",
      "+-----+--------------------+--------------------+------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_palabras.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "270f9e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SWS = StopWordsRemover(inputCol=\"token_text\" ,outputCol=\"SWS_\") \n",
    "swsd = SWS.transform(q_palabras)\n",
    "### Sacamos las StopWords para luego restar las palabras vs palabras sin StopWords y obtener la cantidad\n",
    "### de StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "38bc301b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+------+--------+--------------------+\n",
      "|class|       response_text|          token_text|length|palabras|                SWS_|\n",
      "+-----+--------------------+--------------------+------+--------+--------------------+\n",
      "| spam|want 2 get laid t...|[want, 2, get, la...|   162|      28|[want, 2, get, la...|\n",
      "| spam|call germany for ...|[call, germany, f...|   128|      24|[call, germany, 1...|\n",
      "|  ham|i taught that ran...|[i, taught, that,...|   143|      27|[taught, ranjith,...|\n",
      "+-----+--------------------+--------------------+------+--------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swsd.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "0434bf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####                                                     AQUI agrupar por Flagged\n",
    "Q_stops1 = swsd.select(\"*\", explode(\"SWS_\").alias(\"boom\")).groupBy(\"class\",\"response_text\", \"SWS_\",\"palabras\",\"length\").agg(count(\"boom\").alias(\"q_de_stops\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "fb0f537e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------+------+----------+\n",
      "|class|       response_text|                SWS_|palabras|length|q_de_stops|\n",
      "+-----+--------------------+--------------------+--------+------+----------+\n",
      "|  ham|oops sorry. just ...|[oops, sorry., ch...|      22|   110|        11|\n",
      "|  ham|mmmmm ... i loved...|[mmmmm, ..., love...|      35|   158|        18|\n",
      "|  ham|aww thats the fir...|[aww, thats, firs...|      22|    99|        16|\n",
      "+-----+--------------------+--------------------+--------+------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q_stops1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "5929f088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-----------------+------------------+\n",
      "|class|     avg(palabras)|      avg(length)|   avg(q_de_stops)|\n",
      "+-----+------------------+-----------------+------------------+\n",
      "|  ham|17.772002200220022|82.27970297029702| 10.18124312431243|\n",
      "| spam|27.503852080123266|  138.42218798151|17.486902927580893|\n",
      "+-----+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Visualizar data\n",
    "Q_stops1.groupBy(\"class\").mean().show()\n",
    "\n",
    "##### 17-10 = 7 stopers avg (ham)      -----       27-17=10 stopers avg (spam)\n",
    "#### La resta es el AVG de StopWords por clase (clase: spam, no spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "5e548386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+------------------+\n",
      "|class|      avg(length)|     avg(palabras)|\n",
      "+-----+-----------------+------------------+\n",
      "|  ham|82.27970297029702|17.772002200220022|\n",
      "| spam|  138.42218798151|27.503852080123266|\n",
      "+-----+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizar data\n",
    "q_palabras.groupBy(\"class\").mean().show()\n",
    "\n",
    "#### Los flageados suelen tener más cantidad de palabras \n",
    "#### Importa poco xq ya medimos la cantidad de letras, pero el ratio es similar y sirve para observar la Q de stopwords\n",
    "\n",
    "### Solo usaremos como feature la cantidad de stopwords y length avg (letras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "6f4b0b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+------+--------+\n",
      "|class|       response_text|          token_text|length|palabras|\n",
      "+-----+--------------------+--------------------+------+--------+\n",
      "| spam|want 2 get laid t...|[want, 2, get, la...|   162|      28|\n",
      "| spam|call germany for ...|[call, germany, f...|   128|      24|\n",
      "|  ham|i taught that ran...|[i, taught, that,...|   143|      27|\n",
      "+-----+--------------------+--------------------+------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizar data\n",
    "q_palabras.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "1322559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restamos palabras - palabras_sin_stopwords y obtenemos la cantidad de stopwords\n",
    "df2 = Q_stops1.withColumn(\"StopWordsQ\",col(\"palabras\")-col(\"q_de_stops\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "8c89a2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------+------+----------+----------+\n",
      "|class|       response_text|                SWS_|palabras|length|q_de_stops|StopWordsQ|\n",
      "+-----+--------------------+--------------------+--------+------+----------+----------+\n",
      "|  ham|oops sorry. just ...|[oops, sorry., ch...|      22|   110|        11|        11|\n",
      "|  ham|mmmmm ... i loved...|[mmmmm, ..., love...|      35|   158|        18|        17|\n",
      "|  ham|aww thats the fir...|[aww, thats, firs...|      22|    99|        16|         6|\n",
      "+-----+--------------------+--------------------+--------+------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "c467b81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- response_text: string (nullable = true)\n",
      " |-- length: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "05a37bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- response_text: string (nullable = true)\n",
      " |-- SWS_: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- palabras: long (nullable = false)\n",
      " |-- length: integer (nullable = true)\n",
      " |-- q_de_stops: long (nullable = false)\n",
      " |-- StopWordsQ: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "d67d7fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-----------------+------------------+------------------+\n",
      "|class|     avg(palabras)|      avg(length)|   avg(q_de_stops)|   avg(StopWordsQ)|\n",
      "+-----+------------------+-----------------+------------------+------------------+\n",
      "|  ham|17.772002200220022|82.27970297029702| 10.18124312431243| 7.590759075907591|\n",
      "| spam|27.503852080123266|  138.42218798151|17.486902927580893|10.016949152542374|\n",
      "+-----+------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy(\"class\").mean().show()\n",
    "### Vemos que el average de cantidad de stopwords es igual al visto anteriormente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d916cb4e",
   "metadata": {},
   "source": [
    "#### Ahora con este DF completo se arma el pipeline,\n",
    "#### Ojo que este ya está TOKENIZADO y Sin STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "0782fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "270554ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Se crea el Pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"response_text\" ,outputCol=\"token_text\")\n",
    "stop_remove=StopWordsRemover(inputCol=\"token_text\" ,outputCol=\"stop_text\")\n",
    "#bigrams = NGram(n=24, inputCol=\"stop_text\",outputCol=\"bigrams\") ### con 2 no sirve Reduce mucho el accuracy y f1 score\n",
    "###       #####                 #####                 #####      ### con 10+ queda igual que la linea base\n",
    "count_vect=CountVectorizer(inputCol=\"stop_text\" ,outputCol=\"c_vec\")\n",
    "idf=IDF(inputCol=\"c_vec\" ,outputCol=\"tf_idf\")\n",
    "\n",
    "flag_nflag_numeric=StringIndexer(inputCol=\"class\" ,outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "3ddbe7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "cbf068ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eliges que Features (x's) usar en el modelo para entrenarlo, en este caso\n",
    "## usamos otras features como length y cantidad de stopwords, para ver si se le da mas poder predictivo al modelo\n",
    "\n",
    "clean_up = VectorAssembler(inputCols=[\"tf_idf\",\"length\",\"StopWordsQ\"],outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "85ae61de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "0a64096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb  = NaiveBayes()\n",
    "lr  = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "dcd0e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "d07d3299",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creas el Pipeline, que te hace datacleanin, tokeniza y vectoriza, etc.\n",
    "## Aumenta el bigrams o remuevelo si lo usas o no\n",
    "data_prep_pipe = Pipeline(stages = [flag_nflag_numeric, tokenizer,stop_remove, count_vect,\n",
    "                          idf,clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "aac045c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = data_prep_pipe.fit(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "3a05134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = cleaner.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "0aa14c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data.select(\"label\",\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "c2efb958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(12866,[9,184,216...|\n",
      "|  0.0|(12866,[22,49,50,...|\n",
      "|  0.0|(12866,[0,24,25,5...|\n",
      "|  0.0|(12866,[26,123,20...|\n",
      "+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:07:12 WARN DAGScheduler: Broadcasting large task binary with size 1125.0 KiB\n"
     ]
    }
   ],
   "source": [
    "clean_data.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "677cb8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training,test=clean_data.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "972c5279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:07:53 WARN DAGScheduler: Broadcasting large task binary with size 1138.6 KiB\n",
      "22/09/03 22:07:54 WARN DAGScheduler: Broadcasting large task binary with size 1115.7 KiB\n"
     ]
    }
   ],
   "source": [
    "flag_detector = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "88a6f836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- response_text: string (nullable = true)\n",
      " |-- SWS_: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- palabras: long (nullable = false)\n",
      " |-- length: integer (nullable = true)\n",
      " |-- q_de_stops: long (nullable = false)\n",
      " |-- StopWordsQ: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "23e6a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = flag_detector.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "710faa2f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(12866,[0,1,4,16,...|[-677.55956522220...|[1.0,1.7476945285...|       0.0|\n",
      "|  0.0|(12866,[0,1,7,10,...|[-870.73954884635...|[1.0,9.1773894723...|       0.0|\n",
      "|  0.0|(12866,[0,1,20,33...|[-395.07555018710...|[1.0,1.2979971771...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:07:55 WARN DAGScheduler: Broadcasting large task binary with size 1349.4 KiB\n"
     ]
    }
   ],
   "source": [
    "### Veamos como predice\n",
    "test_results.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "9299e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Observaremos el accuracy f1 etc\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "acc_eval=MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "2a6303a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:07:55 WARN DAGScheduler: Broadcasting large task binary with size 1353.2 KiB\n"
     ]
    }
   ],
   "source": [
    "ff1 = acc_eval.evaluate(test_results,{acc_eval.metricName: \"f1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "0c411cfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.930390966110323"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff1\n",
    "## f1 de 93 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "426ac3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:07:56 WARN DAGScheduler: Broadcasting large task binary with size 1353.2 KiB\n",
      "22/09/03 22:07:57 WARN DAGScheduler: Broadcasting large task binary with size 1353.2 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9261744966442953"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator= MulticlassClassificationEvaluator()\n",
    "\n",
    "evaluator.setPredictionCol(\"prediction\")\n",
    "\n",
    "evaluator.evaluate(test_results)\n",
    "\n",
    "evaluator.evaluate(test_results, {evaluator.metricName: \"accuracy\"})\n",
    "## Accuracy de 926"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "7bf3ea71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:07:59 WARN DAGScheduler: Broadcasting large task binary with size 1138.1 KiB\n",
      "22/09/03 22:07:59 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:07:59 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:00 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:00 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:00 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:00 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:01 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:01 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:01 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:01 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:02 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:02 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:02 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:02 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:03 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:03 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:03 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:03 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:03 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:04 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:04 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:04 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:04 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:05 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:05 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:05 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:05 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:06 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:06 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:06 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:06 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:07 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:07 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:07 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:07 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:08 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:08 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:08 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:09 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:09 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n",
      "22/09/03 22:08:09 WARN DAGScheduler: Broadcasting large task binary with size 1139.7 KiB\n"
     ]
    }
   ],
   "source": [
    "### Ahora intenamos con Log Reg\n",
    "flag_detector = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "56c229dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = flag_detector.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "df01eab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:08:11 WARN DAGScheduler: Broadcasting large task binary with size 1259.0 KiB\n"
     ]
    }
   ],
   "source": [
    "ff1 = acc_eval.evaluate(test_results,{acc_eval.metricName: \"f1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "7a5e8a5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.968944970314222"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff1\n",
    "### Con log reg aumenta a 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "b1d96487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:08:12 WARN DAGScheduler: Broadcasting large task binary with size 1259.0 KiB\n",
      "22/09/03 22:08:13 WARN DAGScheduler: Broadcasting large task binary with size 1259.0 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9697986577181208"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator= MulticlassClassificationEvaluator()\n",
    "\n",
    "evaluator.setPredictionCol(\"prediction\")\n",
    "\n",
    "evaluator.evaluate(test_results)\n",
    "\n",
    "evaluator.evaluate(test_results, {evaluator.metricName: \"accuracy\"})\n",
    "### Con Acc aumenta a 96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d978e",
   "metadata": {},
   "source": [
    "## Añadianedo lematizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "7677c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "e00d5a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "dd3ad57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "            return 'a'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "            return 'v'\n",
    "    elif treebank_tag.startswith('N'):\n",
    "            return 'n'\n",
    "    elif treebank_tag.startswith('R'):\n",
    "            return 'r'\n",
    "    else:\n",
    "    # As default pos in lemmatization is Noun\n",
    "        return 'n'\n",
    "\n",
    "\n",
    "def lemmatize1(data_str):\n",
    "    # expects a string\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    #text = data_str.split()\n",
    "    tagged_words = nltk.pos_tag(data_str)\n",
    "    for word in tagged_words:\n",
    "        lemma = lmtzr.lemmatize(word[0], get_wordnet_pos(word[1]))\n",
    "        if list_pos == 0:\n",
    "            cleaned_str = lemma\n",
    "        else:\n",
    "            cleaned_str = cleaned_str + ' ' + lemma\n",
    "        list_pos += 1\n",
    "    return cleaned_str\n",
    "\n",
    "sparkLemmer1 = udf(lambda x: lemmatize1(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "a069915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se tokeniza antes de aplicar el lematizer\n",
    "tokenizer = Tokenizer(inputCol=\"response_text\" ,outputCol=\"token_text\") \n",
    "tokenized = tokenizer.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "8194d8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------+------+----------+----------+--------------------+\n",
      "|class|       response_text|                SWS_|palabras|length|q_de_stops|StopWordsQ|          token_text|\n",
      "+-----+--------------------+--------------------+--------+------+----------+----------+--------------------+\n",
      "|  ham|oops sorry. just ...|[oops, sorry., ch...|      22|   110|        11|        11|[oops, sorry., ju...|\n",
      "|  ham|mmmmm ... i loved...|[mmmmm, ..., love...|      35|   158|        18|        17|[mmmmm, ..., i, l...|\n",
      "|  ham|aww thats the fir...|[aww, thats, firs...|      22|    99|        16|         6|[aww, thats, the,...|\n",
      "+-----+--------------------+--------------------+--------+------+----------+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "fba15409",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se lematiza y se selecciona las cols que te quieras quedar\n",
    "lema_df= tokenized.select(\"class\",\"response_text\",\"length\", \"SWS_\",\"length\", \"StopWordsQ\",\n",
    "                                 sparkLemmer1(\"token_text\").alias('lems'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "0a415dd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2514:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+--------------------+------+----------+--------------------+\n",
      "|class|       response_text|length|                SWS_|length|StopWordsQ|                lems|\n",
      "+-----+--------------------+------+--------------------+------+----------+--------------------+\n",
      "|  ham|oops sorry. just ...|   110|[oops, sorry., ch...|   110|        11|oops sorry. just ...|\n",
      "|  ham|mmmmm ... i loved...|   158|[mmmmm, ..., love...|   158|        17|mmmmm ... i love ...|\n",
      "|  ham|aww thats the fir...|    99|[aww, thats, firs...|    99|         6|aww thats the fir...|\n",
      "|  ham|no need to buy lu...|    47|[need, buy, lunch...|    47|         4|no need to buy lu...|\n",
      "|  ham|o we cant see if ...|    74|[o, cant, see, jo...|    74|         7|o we cant see if ...|\n",
      "|  ham|nope thats fine. ...|    41|[nope, thats, fin...|    41|         3|nope thats fine. ...|\n",
      "|  ham|those cocksuckers...|   133|[cocksuckers., ma...|   133|         9|those cocksuckers...|\n",
      "|  ham|jade its paul. y ...|    89|[jade, paul., y, ...|    89|         6|jade it paul. y d...|\n",
      "|  ham|thanks for ve lov...|    38|[thanks, ve, love...|    38|         2|thanks for ve lov...|\n",
      "|  ham|its really gettin...|    48|[really, getting,...|    48|         4|it really get me ...|\n",
      "|  ham|no. its not speci...|   114|[no., specialisat...|   114|        10|no. it not specia...|\n",
      "|  ham|im freezing and c...|    33|[im, freezing, cr...|    33|         1|im freezing and c...|\n",
      "|  ham|yup i thk they r ...|   102|[yup, thk, r, e, ...|   102|         8|yup i thk they r ...|\n",
      "|  ham|no he didnt. spri...|    41|[didnt., spring, ...|    41|         3|no he didnt. spri...|\n",
      "|  ham|where are you ? y...|    62|[?, said, woke, ....|    62|        10|where be you ? yo...|\n",
      "|  ham|yes watching foot...|    70|[yes, watching, f...|    70|         4|yes watch footie ...|\n",
      "| spam|you have won! as ...|   134|[won!, valued, vo...|   134|        12|you have won! a a...|\n",
      "|  ham|, im .. on the sn...|   146|[,, im, .., snowb...|   146|        10|, im .. on the sn...|\n",
      "|  ham|moji just informe...|    55|[moji, informed, ...|    55|         5|moji just inform ...|\n",
      "|  ham|was just about to...|   110|[ask., keep, one....|   110|        13|be just about to ...|\n",
      "+-----+--------------------+------+--------------------+------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lema_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "a4414c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Se crea el Pipeline\n",
    "### Ahora no hay tokenizer, entra el \"lems\" del lema_df\n",
    "tokenizer = Tokenizer(inputCol=\"lems\" ,outputCol=\"token_text\") \n",
    "stop_remove=StopWordsRemover(inputCol=\"token_text\" ,outputCol=\"stop_text\")\n",
    "#bigrams = NGram(n=3, inputCol=\"stop_text\",outputCol=\"bigrams\")\n",
    "count_vect=CountVectorizer(inputCol=\"stop_text\" ,outputCol=\"c_vec\")\n",
    "idf=IDF(inputCol=\"c_vec\" ,outputCol=\"tf_idf\") \n",
    "\n",
    "flag_nflag_numeric=StringIndexer(inputCol=\"class\" ,outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "529daf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "d89ef1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eliges que Features (x's) usar en el modelo para entrenarlo\n",
    "clean_up = VectorAssembler(inputCols=[\"tf_idf\",\"length\",\"StopWordsQ\"],outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "e732c340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "2d485e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb  = NaiveBayes()\n",
    "lr  = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "e912a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "47dcc2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creas el Pipeline, que t ehace datacleanin, tokeniza y vectoriza, etc.\n",
    "data_prep_pipe = Pipeline(stages = [flag_nflag_numeric,tokenizer,stop_remove,count_vect,\n",
    "                          idf,clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "7ae58bb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cleaner = data_prep_pipe.fit(lematizar_eldf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "ecffbae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = cleaner.transform(lematizar_eldf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "84531f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data.select(\"label\",\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "47f45807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(11994,[10,85,168...|\n",
      "|  0.0|(11994,[0,3,24,30...|\n",
      "|  0.0|(11994,[0,19,22,2...|\n",
      "|  0.0|(11994,[26,101,16...|\n",
      "+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2547:>                                                       (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "clean_data.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "e1916124",
   "metadata": {},
   "outputs": [],
   "source": [
    "training,test=clean_data.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "359e83e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:10:10 WARN DAGScheduler: Broadcasting large task binary with size 1078.7 KiB\n",
      "22/09/03 22:10:16 WARN DAGScheduler: Broadcasting large task binary with size 1052.6 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flag_detector = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "22e5119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = flag_detector.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "9529cb31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:10:17 WARN DAGScheduler: Broadcasting large task binary with size 1275.7 KiB\n",
      "[Stage 2566:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(11994,[0,1,3,4,7...|[-964.94723188897...|[1.0,1.3294001706...|       0.0|\n",
      "|  0.0|(11994,[0,1,3,4,7...|[-822.47970638325...|[1.0,6.3716408349...|       0.0|\n",
      "|  0.0|(11994,[0,1,3,11,...|[-756.50714821824...|[1.0,1.1209148547...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_results.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "51f79e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Observaremos el accuracy f1 etc\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "f1_eval=MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "bdc6f05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:10:22 WARN DAGScheduler: Broadcasting large task binary with size 1279.5 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ff1 = f1_eval.evaluate(test_results,{acc_eval.metricName: \"f1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "d117a86a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9204199024221255"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "73acf728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:10:27 WARN DAGScheduler: Broadcasting large task binary with size 1279.5 KiB\n",
      "22/09/03 22:10:32 WARN DAGScheduler: Broadcasting large task binary with size 1279.5 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9148936170212766"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator= MulticlassClassificationEvaluator()\n",
    "\n",
    "evaluator.setPredictionCol(\"prediction\")\n",
    "\n",
    "evaluator.evaluate(test_results)\n",
    "\n",
    "evaluator.evaluate(test_results, {evaluator.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5f6dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### intentamos con LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "b0fb1c13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:10:37 WARN DAGScheduler: Broadcasting large task binary with size 1078.0 KiB\n",
      "22/09/03 22:10:42 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:46 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:47 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:47 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:47 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:47 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:48 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:48 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:48 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:48 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:49 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:49 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:49 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:49 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:49 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:50 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:50 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:50 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:51 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:51 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:51 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:51 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:51 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:52 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:52 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:52 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:52 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:53 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:53 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:53 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n",
      "22/09/03 22:10:53 WARN DAGScheduler: Broadcasting large task binary with size 1079.6 KiB\n"
     ]
    }
   ],
   "source": [
    "flag_detector = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "0ba7b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = flag_detector.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "2d1667a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### test_results.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "facf391c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:10:55 WARN DAGScheduler: Broadcasting large task binary with size 1192.0 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ff1 = acc_eval.evaluate(test_results,{acc_eval.metricName: \"f1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "ecb3dc22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9719526505432493"
      ]
     },
     "execution_count": 663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff1\n",
    "## Mejora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "2d22bc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/04 00:28:19 WARN DAGScheduler: Broadcasting large task binary with size 1265.1 KiB\n",
      "22/09/04 00:28:19 WARN DAGScheduler: Broadcasting large task binary with size 1265.1 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9859484777517564"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator= MulticlassClassificationEvaluator()\n",
    "\n",
    "evaluator.setPredictionCol(\"prediction\")\n",
    "\n",
    "evaluator.evaluate(test_results)\n",
    "\n",
    "evaluator.evaluate(test_results, {evaluator.metricName: \"accuracy\"})\n",
    "\n",
    "### LogReg Mejora otra vez, sin embargo, con lematizer, sin lematizer y sin o con mas features el modelo no mejora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "a35d8c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/03 22:12:53 WARN DAGScheduler: Broadcasting large task binary with size 1182.3 KiB\n",
      "22/09/03 22:13:00 WARN DAGScheduler: Broadcasting large task binary with size 1182.2 KiB\n",
      "/home/mauricio/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "22/09/03 22:13:04 WARN DAGScheduler: Broadcasting large task binary with size 1157.5 KiB\n",
      "22/09/03 22:13:05 WARN DAGScheduler: Broadcasting large task binary with size 1167.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[700.   1.]\n",
      " [ 22. 123.]]\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "preds_and_labels = test_results.select(['prediction',\n",
    "                                       'label']).withColumn('label',\n",
    "                                                        F.col('label').cast(FloatType())).orderBy('prediction')\n",
    "\n",
    "#select only prediction and label columns\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebcb26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Predice muy bien los 0's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "75ccfcfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9838369641602248"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2*(700/(701)) * (700/(722))) / ((700/(701))+(700/(722)))\n",
    "## f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "id": "8d1eb8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/04 00:32:33 WARN DAGScheduler: Broadcasting large task binary with size 1256.7 KiB\n"
     ]
    }
   ],
   "source": [
    "lr_metric = MulticlassMetrics(test_results[\"label\",\"prediction\"].rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "3b61b67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9859484777517564\n",
      "Precision 0.9209302325581395\n",
      "Recall 0.9658536585365853\n",
      "F1score 0.9428571428571427\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy\", lr_metric.accuracy)\n",
    "print(\"Precision\", lr_metric.precision(1.0))\n",
    "print(\"Recall\", lr_metric.recall(1.0))\n",
    "print(\"F1score\", lr_metric.fMeasure(1.0))\n",
    "### Reusltados buenos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "7804a5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9985734664764622\n",
      "0.9695290858725761\n"
     ]
    }
   ],
   "source": [
    "print(700/(701)) P\n",
    "print((700/(722))) R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c8e025",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "##### Podemos concluir que sin/con lematizer y sin/con nuevas features (cantidad stopwords y length)\n",
    "##### el modelo fue igual en todos los casos, pero se mejora si en vez de usar NBayes se usa una Regresion Logistica.\n",
    "##### Tambien la matriz de confusion nos demuesta que el modelo tiene un comportamiento correcto respecto al Recall, Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c71f86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
